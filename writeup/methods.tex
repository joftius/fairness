% !TEX root=ricardo_draft.tex

As discussed in the previous Section, we need to relate $\hat Y$ to
$Y$ if the predictor is to be useful, and that we will restrict to
$\hat Y$ to be a (parameterized) function of the non-descendants of
$A$ in the causal graph. An algorithm is introduced in Section
\ref{sec:algorithm}, followed by a discussion of the assumptions that
can be used to express counterfactuals.

\subsection{Algorithm}
\label{sec:algorithm}

Let $\hat Y \equiv g_\theta(U, X_{\nsucc A})$ be a predictor
parameterized by $\theta$, such as a logistic regression or a neural
network, and where $X_{\nsucc A} \subseteq X$ are non-descendants of
$A$. Given a loss function $l(\cdot, \cdot)$ such as squared loss of
log-likelihood, and training data $\mathcal D \equiv \{(A^{(i)}, X^{(i)}, Y^{(i)})\}$
for $i = 1, 2, \dots, n$, we define $L(\theta) \equiv \sum_{i =
  1}^n \mathbb E[l(y^{(i)}, g_\theta(U^{(i)}, x^{(i)}_{\nsucc
    A})\ |\ x^{(i)}, a^{(i)}] / n$ as the empirical loss to be
minimized with respect to $\theta$.  Each expectation is with respect
to random variable $U^{(i)} \sim P_{\mathcal M}(U\ |\ x^{(i)},
a^{(i)})$ where $P_{\mathcal M}(U\ |\ x, a)$ is the conditional
distribution of the background variables as given by a causal model
$\mathcal M$ that is available by assumption. If this expectation
cannot be calculated analytically, Markov chain Monte Carlo (MCMC) can
be used to approximate it, resulting in the following algorithm.
  
\begin{algorithmic}[1]
\Procedure{FairLearning}{$\mathcal D, \mathcal M$}\Comment{Learned parameters $\hat \theta$}  
  \State For each data point $i \in \mathcal D$, sample $m$ MCMC samples
  $U_1^{(i)}, \dots, U_m^{(i)} \sim P_{\mathcal M}(U\ |\ x^{(i)},a^{(i)})$.
  \State Let $\mathcal D'$ be the augmented dataset where each point
  $(a^{(i)}, x^{(i)}, y^{(i)})$ in $\mathcal D$ is replaced with the corresponding $m$ points
  $\{(a^{(i)}, x^{(i)}, y^{(i)}, u_j^{(i)})\}$.
  \State $\hat \theta \leftarrow \mathrm{argmin}_\theta \sum_{i' \in \mathcal D'}
                                   l(y^{(i')}, g_\theta(U^{(i')}, x^{(i')}_{\nsucc A}))$.
\EndProcedure
\end{algorithmic}

At prediction time, we report $\tilde Y \equiv \mathbb E[\hat Y(U^\star,
  x^\star_{\nsucc A})\ |\ x^\star, a^\star]$ for a new data point $(a^\star,
x^\star)$.

\noindent{\bf Deconvolution perspective.} The algorithm can be
understood as a deconvolution approach that, given observables $A \cup
X$, extracts its latent sources and pipeline them into a predictive
model. We advocate that \emph{counterfactual assumptions must underlie
  all approaches that claim to extract the sources of variation of the
  data as ``fair'' latent components}. As an example,
\citet{louizos2015variational} start from the DAG $A \rightarrow X
\leftarrow U$ to extract $P(U\ |\ X, A)$. As $U$ and $A$ are not
independent given $X$ in this representation, a type of penalization
is enforced to create a posterior $P_{fair}(U\ | A, X)$ that is close
to the model posterior $P(U\ |\ A, X)$ while satisfying $P_{fair}(U\ |
A = a, X) \approx P_{fair}(U\ | A = a', X)$. But {\it this is neither
  necessary nor sufficient for counterfactual fairness}. The model for
$X$ given $A$ and $U$ must be justified by a causal mechanism, and
that being the case, $P(U\ |\ A, X)$ requires no postprocessing. As a
matter of fact, model $\mathcal M$ can be learned by penalizing
empirical dependence measures between $U$ and $A$ given $X$
(e.g. \citet{mooij:09}), but this concerns $\mathcal M$ and not $\hat Y$,
and is motivated by explicit assumptions about structural equations,
as described next.

\subsection{Designing the Input Causal Model}
\label{sec:limit-guide-model}

Model $\mathcal M$ must be provided to algorithm {\sc FairLearning}.
Causal models always require strong assumptions, but counterfactuals
in particular are typically presented in terms of structural equations
which are in general unfalsifiable. We point out that we do not need
to specify a fully deterministic model, and structural equations can
be relaxed as conditional distributions. In particular, the concept of
counterfactual fairness holds under three levels of assumptions of
increasing strength:

\noindent {\bf Level 1.}  Build $\hat Y$ using only the observable
non-descendants of $A$.  This only requires partial causal ordering
and no further causal assumptions, but in many problems there will be
few, if any, observables which are not descendants of demographic
factors.
  
\noindent {\bf Level 2.} We postulate background latent variables that
act as non-deterministic causes of observable variables, based on
explicit domain knowledge and learning algorithms\footnote{In some
  domains, it is actually common to build a model entirely around
  latent constructs with few or no observable parents nor connections
  among observed variables \citep{bol:89}.}. Information about $X$ is
passed to $\hat Y$ via $P(U\ |\ x, a)$.

\noindent {\bf Level 3.} We postulate a fully deterministic model with
latent variables. For instance, the distribution $P(V_i\ |\ V_1,
\dots, V_{i - 1})$ can be treated as an additive error model, $V_i
\!=\! f_i(V_1, \dots, V_{i - 1}) \!+\! e_i$ \citep{peters:14}. The
error term $e_i$ then becomes an input to $\hat Y$ as calculated from
the observed variables. This maximizes the information extracted by
the fair predictor $\hat Y$.

%%%%%%%%%%% ICML
%\begin{itemize}
%\item[Level 1] Given a causal DAG, build $\hat Y$
%  using as covariates only the observable variables  not
%  descendants of the protected attributes $A$. This
%  requires information about the DAG, but no
%  assumptions about structural equations or priors over background
%  variables. %  Here
%  % $\hat Y$ is not a function of $U$ but a function
%  % of the maximal subset of $X$ that does not contain descendants of $A$;
%\item[Level 2] Level 1 ignores much information, particularly if the protected
%  attributes are typical attributes such as race or sex,
%  which are parents of many other variables. To include information
%  from descendants of $A$, we postulate background latent variables
%  that act as causes of observable variables, based on explicit domain
%  knowledge and learning algorithms\footnote{In some domains, it is
%    actually common to build a model entirely around latent constructs
%    with few or no observable parents nor connections among observed
%    variables \citep{bol:89}.}. Information from $X$ will propagate to
%  the latent variables by conditioning.% As these
%  % variables are not descendants of $A$, they can be used to fairly predict
%  % $\hat Y$.  Conditioning on descendants of $A$ propagates information
%  % from $X$ to them. This dependency of each $V_i$ on its parents can be
%  % probabilistic.
%  % , instead of being given by a structural equation, as in the
%  % previous section;
% \item[Level 3] In Level 2, the model factorizes as a
%   general DAG, and each node follows a non-degenerate
%   distribution given observed and latent variables. % In the final
%   % set of assumptions,
%   In this level, we remove all randomness from the conditional
%   distributions obtaining a full decomposition $(U, V, F)$ of the
%   model. % Default assumptions %partially independent of the domain,
%   % might be invoked.
%   For instance, the distribution
%   $p(V_i\ |\ V_1, \dots, V_{i - 1})$ can be treated as an additive
%   error model, $V_i \!=\! f_i(V_1, \dots, V_{i - 1}) \!+\! e_i$
%   \citep{peters:14}. The error term $e_i$ then becomes an input
%   to $\hat Y$ after conditioning on the observed variables. This
%   maximizes the information extracted
%   by the fair predictor $\hat Y$.
%\end{itemize}
%%%%%%%%%% END ICML

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "ricardo_draft"
%%% End:
