% !TEX root=ricardo_draft.tex

As discussed in the previous Section, we need to relate $\hat Y$ to
$Y$ if the predictor is to be useful, and  we restrict
$\hat Y$ to be a (parameterized) function of the non-descendants of
$A$ in the causal graph following
Lemma~\ref{lem:nondescend}. We next introduce an algorithm, then
discuss assumptions that
can be used to express counterfactuals.

\subsection{Algorithm}
\label{sec:algorithm}

Let $\hat Y \equiv g_\theta(U, X_{\nsucc A})$ be a predictor
parameterized by $\theta$, such as a logistic regression or a neural
network, and where $X_{\nsucc A} \subseteq X$ are non-descendants of
$A$. Given a loss function $l(\cdot, \cdot)$ such as squared loss or
log-likelihood, and training data $\mathcal D \equiv \{(A^{(i)}, X^{(i)}, Y^{(i)})\}$
for $i = 1, 2, \dots, n$, we define $L(\theta) \equiv \sum_{i =
  1}^n \mathbb E[l(y^{(i)}, g_\theta(U^{(i)}, x^{(i)}_{\nsucc
    A}))\ |\ x^{(i)}, a^{(i)}] / n$ as the empirical loss to be
minimized with respect to $\theta$.  Each expectation is with respect
to random variable $U^{(i)} \sim P_{\mathcal M}(U\ |\ x^{(i)},
a^{(i)})$ where $P_{\mathcal M}(U\ |\ x, a)$ is the conditional
distribution of the background variables as given by a causal model
$\mathcal M$ that is available by assumption. If this expectation
cannot be calculated analytically, Markov chain Monte Carlo (MCMC) can
be used to approximate it as in the following algorithm.
  
\begin{algorithmic}[1]
\Procedure{FairLearning}{$\mathcal D, \mathcal M$}\Comment{Learned parameters $\hat \theta$}  
  \State For each data point $i \in \mathcal D$, sample $m$ MCMC samples
  $U_1^{(i)}, \dots, U_m^{(i)} \sim P_{\mathcal M}(U\ |\ x^{(i)},a^{(i)})$.
  \State Let $\mathcal D'$ be the augmented dataset where each point
  $(a^{(i)}, x^{(i)}, y^{(i)})$ in $\mathcal D$ is replaced with the corresponding $m$ points
  $\{(a^{(i)}, x^{(i)}, y^{(i)}, u_j^{(i)})\}$.
  \State $\hat \theta \leftarrow \mathrm{argmin}_\theta \sum_{i' \in \mathcal D'}
                                   l(y^{(i')}, g_\theta(U^{(i')}, x^{(i')}_{\nsucc A}))$.
\EndProcedure
\end{algorithmic}

At prediction time, we report $\tilde Y \equiv \mathbb E[\hat Y(U^\star,
  x^\star_{\nsucc A})\ |\ x^\star, a^\star]$ for a new data point $(a^\star,
x^\star)$.

\noindent{\bf Deconvolution perspective.} The algorithm can be
understood as a deconvolution approach that, given observables $A \cup
X$, extracts its latent sources and pipelines them into a predictive
model. We advocate that \emph{counterfactual assumptions must underlie
  all approaches that claim to extract the sources of variation of the
  data as ``fair'' latent components}. As an example,
\citet{louizos2015variational} start from the DAG $A \rightarrow X
\leftarrow U$ to extract $P(U\ |\ X, A)$. As $U$ and $A$ are not
independent given $X$ in this representation, a type of penalization
is enforced to create a posterior $P_{fair}(U\ | A, X)$ that is close
to the model posterior $P(U\ |\ A, X)$ while satisfying $P_{fair}(U\ |
A = a, X) \approx P_{fair}(U\ | A = a', X)$. But {\it this is neither
  necessary nor sufficient for counterfactual fairness}. The model for
$X$ given $A$ and $U$ must be justified by a causal mechanism, and
that being the case, $P(U\ |\ A, X)$ requires no postprocessing. As a
matter of fact, model $\mathcal M$ can be learned by penalizing
empirical dependence measures between $U$ and $pa_i$ for a given $V_i$
(e.g. \citet{mooij:09}), but this concerns $\mathcal M$ and not $\hat Y$,
and is motivated by explicit assumptions about structural equations,
as described next.

\subsection{Designing the Input Causal Model}
\label{sec:limit-guide-model}

Model $\mathcal M$ must be provided to algorithm {\sc FairLearning}.
Although this is well understood, it is worthwhile remembering that
causal models always require strong assumptions, even more so when
making counterfactual claims \cite{dawid:00}. Counterfactuals
assumptions such as structural equations are in general unfalsifiable
even if interventional data for all variables is available. This is
because there are infinitely many structural equations compatible with
the same observable distribution \cite{pearl:00}, be it observational
or interventional. Having passed testable implications, the remaining
components of a counterfactual model should be understood as
conjectures formulated according to the best of our knowledge. Such
models should be deemed provisional and prone to modifications if, for
example, new data containing measurement of variables previously
hidden contradict the current model.

We point out that we do not need to specify a fully deterministic
model, and structural equations can be relaxed as conditional
distributions. In particular, the concept of counterfactual fairness
holds under three levels of assumptions of increasing strength:

\noindent {\bf Level 1.}  Build $\hat Y$ using only the observable
non-descendants of $A$.  This only requires partial causal ordering
and no further causal assumptions, but in many problems there will be
few, if any, observables which are not descendants of protected
demographic factors.
  
\noindent {\bf Level 2.} Postulate background latent variables that
act as non-deterministic causes of observable variables, based on
explicit domain knowledge and learning algorithms\footnote{In some
  domains, it is actually common to build a model entirely around
  latent constructs with few or no observable parents nor connections
  among observed variables \citep{bol:89}.}. Information about $X$ is
passed to $\hat Y$ via $P(U\ |\ x, a)$.

\noindent {\bf Level 3.} Postulate a fully deterministic model with
latent variables. For instance, the distribution $P(V_i\ |\ pa_i)$
can be treated as an additive error model, $V_i
\!=\! f_i(pa_i) \!+\! e_i$ \citep{peters:14}. The
error term $e_i$ then becomes an input to $\hat Y$ as calculated from
the observed variables. This maximizes the information extracted by
the fair predictor $\hat Y$.

\subsection{Further Considerations on Designing the Input Causal Model}
\label{sec:pragmatic}

One might ask what we can lose by defining causal fairness measures involving
only non-counterfactual causal quantities, such as enforcing $P(\hat Y =
1\ |\ do(A = a)) = P(\hat Y = 1\ |\ do(A = a'))$ instead of our
counterfactual criterion. The reason is that the above equation is
only a constraint on an average effect. Obeying this criterion
provides no guarantees against, for example, having half of the
individuals being strongly ``negatively'' discriminated and half of
the individuals strongly ``positively'' discriminated.  We advocate
that, for fairness, society should not be satisfied in pursuing only
counterfactually-free guarantees. While one may be willing to claim
posthoc that the equation above masks no balancing effect so that
individuals receive approximately the same distribution of outcomes,
{\it that itself is just a counterfactual claim in disguise.} Our
approach is to make counterfactual assumptions explicit. When
unfairness is judged to follow only some ``pathways'' in the causal
graph (in a sense that can be made formal, see
\cite{kilbertus:17,nabi:17}), nonparametric assumptions about the
independence of counterfactuals may suffice, as discussed by
\cite{nabi:17}. In general, nonparametric assumptions may not provide
identifiable adjustments even in this case, as also discussed in our
Supplementary Material.  If competing models with different untestable
assumptions are available, there are ways of simultaneously enforcing a notion of
approximate counterfactual fairness in all of them, as introduced by
us in \cite{russell:17}. Other alternatives include exploiting
bounds on the contribution of hidden variables \cite{pearl:16,silva:16}.

Another issue is the interpretation of causal claims involving
demographic variables such as race and sex. Our view is that such
constructs are the result of translating complex events into random
variables and, despite some controversy, we consider counterproductive
to claim that e.g. race and sex cannot be causes. An idealized
intervention on some $A$ at a particular time can be seen as a notational
shortcut to express a conjunction of more specific interventions,
which may be individually doable but jointly impossible in practice.
It is the plausibility of complex, even if impossible to practically
manipulate, causal chains from $A$ to $Y$ that allows us to
claim that unfairness is real \cite{glymour:14}. Experiments for
constructs exist, such as randomizing names in job applications to
make them race-blind. They do not contradict the notion of race as a
cause, and can be interpreted as an intervention on a particular
aspect of the construct ``race,'' such as ``race perception'' (e.g. Section 4.4.4 of
\cite{pearl:16}).

%just a special case of the problem of defining what the measurement of
%a construct is: when we define several measures of what takes for a
%nation to be ``democratic'' we are effectively implying in which ways
%we could intervene on democracy. The combination of such actions
%may not be practical in the same sense that imagining  
%having a democracy has an efffecwe can make it democratic
%\cite{silva:16b}: it is from our choice of no way different from
%making causal claims with other constructs and that in one way or
%another we are dealing with idealizations: in the same way we use . As
%discussed by \cite{glymour:14}, when dealing with variables which
%correspond to a particular point in a sequential processes (``Sue
%would not have her job application rejected today had she been male''
%can be defined counterfactually by changing her biological sex at
%conceptions and everything else in the world up to that point that is
%necessary for her to be applying to that job in the same original
%conditions).

%%%%%%%%%%% ICML
%\begin{itemize}
%\item[Level 1] Given a causal DAG, build $\hat Y$
%  using as covariates only the observable variables  not
%  descendants of the protected attributes $A$. This
%  requires information about the DAG, but no
%  assumptions about structural equations or priors over background
%  variables. %  Here
%  % $\hat Y$ is not a function of $U$ but a function
%  % of the maximal subset of $X$ that does not contain descendants of $A$;
%\item[Level 2] Level 1 ignores much information, particularly if the protected
%  attributes are typical attributes such as race or sex,
%  which are parents of many other variables. To include information
%  from descendants of $A$, we postulate background latent variables
%  that act as causes of observable variables, based on explicit domain
%  knowledge and learning algorithms\footnote{In some domains, it is
%    actually common to build a model entirely around latent constructs
%    with few or no observable parents nor connections among observed
%    variables \citep{bol:89}.}. Information from $X$ will propagate to
%  the latent variables by conditioning.% As these
%  % variables are not descendants of $A$, they can be used to fairly predict
%  % $\hat Y$.  Conditioning on descendants of $A$ propagates information
%  % from $X$ to them. This dependency of each $V_i$ on its parents can be
%  % probabilistic.
%  % , instead of being given by a structural equation, as in the
%  % previous section;
% \item[Level 3] In Level 2, the model factorizes as a
%   general DAG, and each node follows a non-degenerate
%   distribution given observed and latent variables. % In the final
%   % set of assumptions,
%   In this level, we remove all randomness from the conditional
%   distributions obtaining a full decomposition $(U, V, F)$ of the
%   model. % Default assumptions %partially independent of the domain,
%   % might be invoked.
%   For instance, the distribution
%   $p(V_i\ |\ V_1, \dots, V_{i - 1})$ can be treated as an additive
%   error model, $V_i \!=\! f_i(V_1, \dots, V_{i - 1}) \!+\! e_i$
%   \citep{peters:14}. The error term $e_i$ then becomes an input
%   to $\hat Y$ after conditioning on the observed variables. This
%   maximizes the information extracted
%   by the fair predictor $\hat Y$.
%\end{itemize}
%%%%%%%%%% END ICML

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "ricardo_draft"
%%% End:
