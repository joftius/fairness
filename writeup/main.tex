\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
\PassOptionsToPackage{numbers, sort&compress}{natbib}
% before loading nips_2017
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2017}

\usepackage{nips_2017}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2017}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage[pagebackref]{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsthm,amssymb,amsopn,amsmath}
\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
\usepackage{subfigure}
\usepackage{wrapfig}
%\usepackage[nonatbib]{nips_2016}
\newtheorem{assumption}{Assumption}
\newtheorem{define}{Definition}
\newtheorem{thm}{Theorem}
\newtheorem{lem}{Lemma}
\newtheorem{coro}{Corollary}
\title{Coutnerfactual Fairness}
%\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}
\newcommand\cff{{\sc cff}}
% Code blocks%
\usepackage{listings}
\usepackage{color}

\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}

% Code blocks%
\usepackage{listings}
\usepackage{color}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=matlab,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}


\begin{document}
\maketitle
\begin{abstract}
There has been immense recent interest in fairness in machine learning. Algorithms trained on data from the real world, which for historical reasons may produce unfair outcomes, will tend to perpetuate that unfairness through their predictions. For example, racial disparities in the US criminal justice system (cite) may be perpetuated through the predictions of recidivism risk assessment algorithms (cite). A number of recent papers have attempted to address this issue by defining various notions of fairness and proposing algorithms designed to give the best possible predictions while satisfying that definition of fairness. In this work, we leverage the language and methodology developed in the literature on causal inference to address fairness.
%For instance, imagine a bank wishes to train a machine learning model to predict whether or not an individual should be given a loan to buy a house. If the bank simply tries to learn a model that accurately predicts who to loan to solely based on whether the loan will be paid back. 
%Specifically, we would want any model that offers house loans to individuals to not be biased by an individual's race in granting such loans. Or we would like a
%with respect to race, gender, and any other individual attribute 
\end{abstract} 


\section{Introduction}
\input{intro.tex}

\section{Fairness in machine learning}
\input{background.tex}

\section{The counterfactual approach}
\input{counterfact.tex}


\section{Unprejudicial Inference under Causal Structures}
\label{sec-1}

Much as we talk about variables being causally independent of one
another, it's possible to talk of predictions being counter factually
fair.

We say that a predictor of $Y$, $\hat Y(X,A)$ is causally
independent of $A$ if 
%
\[ \hat Y(X,A)=\hat Y(X,A)|\text{do}(A=a) \] 
%
In the general case, a classifier $\hat Y (X)$ that does not directly
depend on $A$ need not be causally independent of $A$ if $X$ depends on $A$.

We are interested in three related questions, illustrated by the
causal diagrams in figure 1. From left to right:

\begin{enumerate}
\item Given a $Y$ causally independent of $A$, can we learn a $\hat Y$,
causally independent of $A$ that accurately predicts $Y$?
\item Given a $Y$ causally \textbf{dependent} on $A$, can we learn a $\hat Y$,
causally independent of $A$ that predicts $Y$ as closely as possible?
\item Given a $Y'$, causally independent of $A$ but unobserved, and an
observed $Y$ which represents $Y$ corrupted by a function of $A$,
can we recover $Y'$?
\end{enumerate}

To simplify this problem, we first consider the linear case in where
variables are distributed Gaussianly and the dependencies are
additive, and related this to previous existing work on orthogonality
in fairness, before considering the more general case.

\subsection{Fair Learning in a Fair World}
\label{sec-1-1}
\subsection{Fair Learning in an Unfair World}
\label{sec-1-2}
\subsection{Recovering Fairness from Corrupted Data}
\label{sec-1-3}


\bibliography{bibliography}
\bibliographystyle{icml2017}

\end{document} 