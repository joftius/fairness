%!TEX root=ricardo_draft.tex
We begin by describing the problem of fair prediction and introduce three of the most popular definitions developed for this task.  We then give background on causal modeling which will act as our `tool-kit' for modeling and defining fairness.

\subsection{Fairness}
Let's imagine a scenario in which predictions must be fair. For instance, imagine a university in the United States (US) would like to know how successful an applicant is going to be after graduation, call this $Y$, given their current incoming features $X$ such as test scores, grade-point average (GPA). To predict success a modeler is given a dataset of $n$ applications with features $\{X^{(1)}, \ldots, X^{(n)} \}$ and measures of graduation success $\{Y^{(1)}, \ldots, Y^{(n)}\}$. However, universities in the US have historically had racial and gender biases in student admission \cite{kane1998racial,kidder2000portia}. Thus, in addition we are given demographic and gender information for each individual $\{A^{(1)}, \ldots, A^{(n)}\}$ that we would like to use to ensure our model is fair.

What does it mean for a model to be fair? To answer this there has been a wealth of recent work aimed at defining fairness (CITE PAPERS). A few popular definitions are (a) Fairness Through Unawareness (FTU), (b) Demographic Parity (DP), (c) Equal Opportunity (EO), and (d) Individual Fairness (IF). These are defined as follows:

\begin{define}[Fairness Through Unawareness (FTU)]
An algorithm is fair so long as the sensitive attribute $A$ is not explicitly used in the decision-making process. Formally, any mapping $\hat{Y}: X \rightarrow Y$ satisfies this definition.
\end{define}

\begin{define}[Demographic Parity (DP)]
An algorithm is fair if its predictions are independent of the sensitive attributes $A$ across the population. Formally, a prediction $\hat{Y}$ satisfies this definition if, 
\begin{align}
P(\hat{Y} | A = 0) = P(\hat{Y} | A = 1). \nonumber
\end{align}
\end{define}

\begin{define}[Equal Opportunity (EO)]
An algorithm is fair if it is equally accurate for every value of the sensitive attribute $A$. Formally, a prediction $\hat{Y}$ satisfies this if,
\begin{align}
P(\hat{Y}=1 | A=0,Y=1) = P(\hat{Y}=1 | A=1,Y=1). \nonumber
\end{align}
\end{define}

\begin{define}[Individual Fairness (IF)]
  An algorithm is fair if it give similar predictions to similar individuals. Formally, if individuals $i$ and $j$ are similar then
\begin{align}
  \hat{Y}(X^{(i)}, A^{(i)}) \approx \hat{Y}(X^{(j)}, A^{(j)}).\nonumber
\end{align}
\end{define}


\subsection{Causal Models and Counterfactuals}
\label{subsec:cmc}
We will follow the framework of \citet{pearl:00}, where a causal
model is a triple $(U, V, F)$ of sets such that
\begin{itemize}
\item $U$ is a set of {\bf background} variables\footnote{These are
  sometimes called {\bf exogenous variables}, but the fact that members of $U$
  might depend on each other is not relevant to what follows.}, which are generated by factors
outside of our potential control;
\item $V$ is a set of {\bf endogenous} variables, where each member is determined by
  other variables in $U \cup V$;
\item $F$ is a set of functions $\{f_1, \dots, f_n\}$, one for each $V_i \in V$, such
that $V_i = f_i(pa_i, U_{pa_i})$, $pa_i \subseteq V \backslash
\{V_i\}$ and $U_{pa_i} \subseteq U$. Such equations are also known as
{\bf structural equations} \citep{bol:89}.
\end{itemize}

The notation ``$pa_i$'' is motivated by the extra assumption that the
model factorizes according to a directed acyclic graph (DAG). That is,
define a directed graph ${\mathcal G}=(U \cup V, \mathcal E )$ where each node corresponds to an
element of $U \cup V$, and each directed edge $V_i \leftarrow X$ is added if
and only if $X \in pa_i \cup U_{pa_i}$. By definition, $\mathcal G$ is
acyclic.

The model is causal in the sense that, for a given probability model
$p(U)$ for the background variables, it gives the distribution of a
subset of $V$ given an {\bf intervention} in another complementary
subset of $V$.  The operational meaning of an intervention on $V_i$ at
value $v$ is the substitution of the equation
$V_i = f_i(pa_i, U_{pa_i})$ with the equation $V_i = v$. This captures
the idea of an agent, external to the system, modifying it. For
instance, this may happen in a randomized controlled trial which
overrides the value of $V_i$ with a treatment that sets it at $v$, a
value chosen at random and independently of any other causes of the
system. The do-calculus of \citet{pearl:00} provides a way to identify
features of interventional distributions %, where possible,
using only estimates of the joint distribution of $V$ and knowledge of
the causal DAG.

Compared to the independence constraints given by a DAG, the full
specification of $F$ requires much stronger assumptions but also leads
to much stronger claims. In particular, it allows for the
calculation of {\bf counterfactual} quantities. % Without going into a
% detailed coverage of the topic
In brief, consider the following counterfactual
statement, ``the value of $Y$ if $X$ had taken value $x$'', for two endogenous
variables $X$ and $Y$ in a causal model. By assumption, the state of
any endogenous variable is fully determined by
the background variables and structural equations. The counterfactual is
modeled as the solution for $Y$ for a given $U = u$ where the equation(s)
for $X$ is (are) replaced with $X = x$.  We denote it by $Y_{X \leftarrow x}(u)$
\cite{pearl:00}, and sometimes as $Y_x$ if the context of the notation is clear.

Counterfactual inference, as specified by a causal model $(U, V, F)$,
is the computation of probabilities
$P(Y_{X \leftarrow x}(U)\ |\ W = w)$, where $W$, $X$ and $Y$ are
subsets of $V$. Inference proceeds in three steps, as explained in
more detail in Chapter 4 of \citet{pearl:16}:
\begin{enumerate}
\item Abduction: for a given prior on $U$, compute the posterior
  distribution of $U$ given the evidence $W = w$;
\item Action: substitute the equations for $X$ with the interventional
  values $x$, resulting in the modified set of equations $F_x$;
\item Prediction: compute the implied distribution on the remaining
  elements of $V$ using $F_x$ and the posterior $P(U\ | W = w)$.
\end{enumerate}



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "ricardo_draft"
%%% End:
