\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
\PassOptionsToPackage{numbers, sort&compress}{natbib}
% before loading nips_2017
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2017}

\usepackage{nips_2017}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2017}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage[pagebackref]{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
%\usepackage{algorithm}
%\usepackage{algorithmic}
\usepackage{algpseudocode}
\usepackage{amsthm,amssymb,amsopn,amsmath}
\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{wrapfig}
%\usepackage[nonatbib]{nips_2016}
\newtheorem{assumption}{Assumption}
\newtheorem{define}{Definition}
\newtheorem{thm}{Theorem}
\newtheorem{lem}{Lemma}
\newtheorem{coro}{Corollary}
\title{Counterfactual Fairness}
%\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}
\newcommand\cff{{\sc cff}}
% Code blocks%
\usepackage{listings}
\usepackage{color}

\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}

% Code blocks%
\usepackage{listings}
\usepackage{color}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=matlab,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}


\begin{document}
\maketitle

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\textsuperscript{*}Equal contribution, author order decided randomly}
%\icmlEqualContribution}
%%\icmlEqualContribution} % otherwise use the standard text.
%\footnotetext{hi}

\begin{abstract} 
% fairness is important
  Machine learning has matured to the point to where it is now being
  considered to automate decisions in loan lending, employee hiring,
  and predictive policing.  In many of these scenarios however,
  previous decisions have been made that are unfairly biased against
  certain subpopulations (e.g., those of a particular race, gender, or
  sexual orientation).  Because this past data is often biased,
  machine learning predictors must account for this to avoid
  perpetuating discriminatory practices (or incidentally making new
  ones).  In this paper, we develop a framework for modeling fairness
  in any dataset using tools from counterfactual inference. We propose
  a definition called \emph{counterfactual fairness} that captures the
  intuition that a decision is fair towards an individual, if it gives
  the same predictions in (a) in the observed world and (b) a world
  where the individual had always belonged to a different demographic
  group, other background causes of the outcome being equal. We
  demonstrate our framework on ao real-world problem of fair
  prediction of law school success.
  %demonstrate our framework on two real-world problems: fair
  %prediction of law school success, and fair modeling of an
  %individual's criminality in policing data.
\end{abstract} 

\section{Contribution}
\label{sec:introduction}
\input{intro}

\section{Background}
\label{sec:background}
\input{background}

%\section{Background}
%\label{sec:related}
%\input{related}

%\section{Causal Models and Counterfactuals}
%\label{sec:background}
%\input{background}

\section{Counterfactual Fairness}
\label{sec:count_fair}
\input{fairness}

\section{Implementing Counterfactual Fairness}
\label{sec:methods}
\input{methods}

\section{Illustration: Law School Success}
\label{sec:experiments}
\input{experiments}

\section{Conclusion}
\label{sec:conclusion}
We have presented a new model of fairness we refer to as {\em
  counterfactual fairness}. It allows us to propose fair algorithms
that, rather than simply ignoring protected attributes, are able to
take into account the different social biases that may arise towards
individuals of a particular race, gender, or sexuality and compensate
for these biases effectively. We experimentally contrasted our
approach with previous unfair approaches and show that our explicit
causal models capture these social biases and make clear the implicit
trade-off between prediction accuracy and fairness in an unfair
world. We propose that fairness should be regulated by explicitly
modeling the causal structure of the world. Criteria based purely on
probabilistic independence cannot satisfy this and are unable to
address \emph{how} unfairness is occurring in the task at hand. By
providing such causal tools for addressing fairness questions we hope
we can provide practitioners with customized techniques for solving a
wide array of fairness modeling problems.

\bibliography{rbas,bibliography}
\bibliographystyle{icml2017}

\input{new_sup}

\end{document} 


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was
% created by Lise Getoor and Tobias Scheffer, it was slightly modified  
% from the 2010 version by Thorsten Joachims & Johannes Fuernkranz, 
% slightly modified from the 2009 version by Kiri Wagstaff and 
% Sam Roweis's 2008 version, which is slightly modified from 
% Prasad Tadepalli's 2007 version which is a lightly 
% changed version of the previous year's version by Andrew Moore, 
% which was in turn edited from those of Kristian Kersting and 
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.  

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "ricardo_draft"
%%% End:
