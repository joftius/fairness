\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
\PassOptionsToPackage{numbers, sort&compress}{natbib}
% before loading nips_2017
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2017}

% to compile a camera-ready version, add the [final] option, e.g.:
\usepackage[final]{nips_2017}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage[pagebackref]{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
%\usepackage{algorithm}
%\usepackage{algorithmic}
\usepackage{algpseudocode}
\usepackage{amsthm,amssymb,amsopn,amsmath}
\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{wrapfig}
%\usepackage[nonatbib]{nips_2016}
\newtheorem{assumption}{Assumption}
\newtheorem{define}{Definition}
\newtheorem{thm}{Theorem}
\newtheorem{lem}{Lemma}
\newtheorem{coro}{Corollary}
\title{Counterfactual Fairness}
%\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}
\newcommand\cff{{\sc cff}}
% Code blocks%
\usepackage{listings}
\usepackage{color}

\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\newcommand*\samethanks[1][\value{footnote}]{\footnotemark[#1]}

% Code blocks%
\usepackage{listings}
\usepackage{color}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=matlab,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}

\author{
  Matt Kusner \thanks{Equal contribution. This work was done while JL was a Research Fellow at the Alan Turing Institute.}\\
  The Alan Turing Institute and \\
  University of Warwick \\
  \texttt{mkusner@turing.ac.uk} \\
  \And
  Joshua Loftus \samethanks\\
  New York University \\
  \texttt{loftus@nyu.edu} \\
  \And
  Chris Russell \samethanks \\
  The Alan Turing Institute and \\
  University of Surrey\\
  \texttt{crussell@turing.ac.uk} \\
  \And
  Ricardo Silva \\
  The Alan Turing Institute and \\
  University College London\\
  \texttt{ricardo@stats.ucl.ac.uk} \\
}

\begin{document}
\maketitle

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\textsuperscript{*}Equal contribution, author order decided randomly}
%\icmlEqualContribution}
%%\icmlEqualContribution} % otherwise use the standard text.
%\footnotetext{hi}

\begin{abstract} 
% fairness is important
  Machine learning can impact people with legal or ethical
  consequences when it is used
  %has matured to the point to where it is now being   considered
  to automate decisions in areas such as insurance, lending, hiring,
  and predictive policing.  In many of these scenarios,
  previous decisions have been made that are unfairly biased against
  certain subpopulations, for example those of a particular race, gender, or
  sexual orientation.  Since this past data may be biased,
  machine learning predictors must account for this to avoid
  perpetuating or creating discriminatory practices.
  In this paper, we develop a framework for modeling fairness
  using tools from causal inference. Our definition of
  \emph{counterfactual fairness} captures the
  intuition that a decision is fair towards an individual if it 
  the same in (a) the actual world and (b) a counterfactual world
  where the individual belonged to a different demographic
  group. We demonstrate our framework on a real-world problem of fair
  prediction of success in law school.
  %demonstrate our framework on two real-world problems: fair
  %prediction of law school success, and fair modeling of an
  %individual's criminality in policing data.
\end{abstract} 

\section{Contribution}
\label{sec:introduction}
\input{intro}

\section{Background}
\label{sec:background}
\input{background}

%\section{Background}
%\label{sec:related}
%\input{related}

%\section{Causal Models and Counterfactuals}
%\label{sec:background}
%\input{background}

\section{Counterfactual Fairness}
\label{sec:count_fair}
\input{fairness}

\section{Implementing Counterfactual Fairness}
\label{sec:methods}
\input{methods}

\section{Illustration: Law School Success}
\label{sec:experiments}
\input{experiments}

\section{Conclusion}
\label{sec:conclusion}
We have presented a new model of fairness we refer to as {\em
  counterfactual fairness}. It allows us to propose algorithms
that, rather than simply ignoring protected attributes, are able to
take into account the different social biases that may arise towards
individuals based on ethically sensitive attributes
and compensate
for these biases effectively. We experimentally contrasted our
approach with previous fairness approaches and show that our explicit
causal models capture these social biases and make clear the implicit
trade-off between prediction accuracy and fairness in an unfair
world. We propose that fairness should be regulated by explicitly
modeling the causal structure of the world. Criteria based purely on
probabilistic independence cannot satisfy this and are unable to
address \emph{how} unfairness is occurring in the task at hand. By
providing such causal tools for addressing fairness questions we hope
we can provide practitioners with customized techniques for solving a
wide array of fairness modeling problems.

\subsubsection*{Acknowledgments}

This work was supported by the Alan Turing Institute under the EPSRC
grant EP/N510129/1. CR acknowledges additional support under the EPSRC Platform Grant EP/P022529/1.
We thank Adrian Weller for insightful feedback, and the anonymous reviewers for helpful comments.

\bibliography{rbas,bibliography}
\bibliographystyle{icml2017}

\newpage
\input{new_sup}

\end{document} 


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was
% created by Lise Getoor and Tobias Scheffer, it was slightly modified  
% from the 2010 version by Thorsten Joachims & Johannes Fuernkranz, 
% slightly modified from the 2009 version by Kiri Wagstaff and 
% Sam Roweis's 2008 version, which is slightly modified from 
% Prasad Tadepalli's 2007 version which is a lightly 
% changed version of the previous year's version by Andrew Moore, 
% which was in turn edited from those of Kristian Kersting and 
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.  

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "ricardo_draft"
%%% End:
