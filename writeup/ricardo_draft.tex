%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% ICML 2017 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Use the following line _only_ if you're still using LaTeX 2.09.
%\documentstyle[icml2017,epsf,natbib]{article}
% If you rely on Latex2e packages, like most moden people use this:
\documentclass{article}

% use Times
\usepackage{times}
% For figures
\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
\usepackage{subfigure} 

% For citations
\usepackage[sort&compress]{natbib}

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsthm,amssymb,amsopn,amsmath}

% As of 2011, we use the hyperref package to produce hyperlinks in the
% resulting PDF.  If this breaks your system, please commend out the
% following usepackage line and replace \usepackage{icml2017} with
% \usepackage[nohyperref]{icml2017} above.
\usepackage{hyperref}

% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Employ the following version of the ``usepackage'' statement for
% submitting the draft version of the paper for review.  This will set
% the note in the first column to ``Under review.  Do not distribute.''
\usepackage{icml2017} 

% Employ this version of the ``usepackage'' statement after the paper has
% been accepted, when creating the final version.  This will set the
% note in the first column to ``Proceedings of the...''
%\usepackage[accepted]{icml2017}
\newtheorem{assumption}{Assumption}
\newtheorem{define}{Definition}
\newtheorem{thm}{Theorem}
\newtheorem{lem}{Lemma}
\newtheorem{coro}{Corollary}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Counterfactual Fairness}

\begin{document} 

\twocolumn[
\icmltitle{Counterfactual Fairness}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2017
% package.

% list of affiliations. the first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% you can specify symbols, otherwise they are numbered in order
% ideally, you should not use this facility. affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Cieua Vvvvv}{goo}
\icmlauthor{Iaesut Saoeu}{ed}

\end{icmlauthorlist}

\icmlaffiliation{goo}{Googol ShallowMind, New London, Michigan, USA}
\icmlaffiliation{ed}{University of Edenborrow, Edenborrow, United Kingdom}

\icmlcorrespondingauthor{Cieua Vvvvv}{c.vvvvv@googol.com}

% You may provide any keywords that you 
% find helpful for describing your paper; these are used to populate 
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Causality,Counter-Factual,Fairness}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.
%\footnotetext{hi}

\begin{abstract} 
% fairness is important
  People have begun to make use of machine learning techniques to
  automate decisions that, historically, have been unfairly biased
  against certain subgroups in the population (e.g., based on race,
  gender, sexual orientation). % This includes making loan decisions,
  % credit scoring, hiring employees, and crime monitoring, among many
  % others.
  Because the historical data is often biased, machine learning
  techniques that are used to make future predictions must account for
  this to avoid perpetuating discriminatory practices. There have been
  a number of recent works towards designing fair classifiers, however
  it is often unclear when to prefer one method over another. In this
  paper, we develop a framework for modeling fairness in any dataset
  using tools from counterfactual inference. We propose a definition
  called \emph{counterfactual fairness} that naturally captures the
  intuition that a decision is fair towards an individual, if it gives
  the same predictions in (a) in the observed world and (b) a world
  where the individual had always belonged to a different demographic
  group. We demonstrate our modeling framework on two real-world
  problems: 1. fair prediction of law school success and 2. fair
  modeling of an individual's latent criminality in stop-and-frisk
  policing data.
\end{abstract} 

\section{Introduction}
\label{introduction}
\input{intro}


\section{Background}
\label{background}
\input{background}

\section{Counterfactual Fairness}
\label{sec:count_fair}
\input{fairness}


\section{Methods and Assessment}
\label{sec:methods}
\input{methods}



\section{Experiments}
\label{sec:experiments}
\input{experiments}

\section{Conclusion}
\label{sec:conclusion}
We have presented a new model of fairness we refer to as {\em
  counterfactual fairness}. This concept allows us to propose fair
algorithms that, rather than simply ignoring protected attributes, are
able to take into account the different opportunities and social biases
that may arise towards and individual of a particular race, gender, or
sexuality and compensate for these biases effectively. We
experimentally contrasted our approach with previous racially blind
approaches and show that our explicit causal models capture these
social biases and make clear the implicit trade-off between
prediction accuracy and fairness in an unfair world.

\bibliography{rbas,bibliography}
\bibliographystyle{icml2017}

\end{document} 


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was
% created by Lise Getoor and Tobias Scheffer, it was slightly modified  
% from the 2010 version by Thorsten Joachims & Johannes Fuernkranz, 
% slightly modified from the 2009 version by Kiri Wagstaff and 
% Sam Roweis's 2008 version, which is slightly modified from 
% Prasad Tadepalli's 2007 version which is a lightly 
% changed version of the previous year's version by Andrew Moore, 
% which was in turn edited from those of Kristian Kersting and 
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.  

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "ricardo_draft"
%%% End:
