%!TEX root=ricardo_draft.tex
% ml is now everywhere
Machine learning has spread to fields as diverse as credit scoring \cite{khandani2010consumer}, crime prediction \cite{brennan2009evaluating}, and loan assessment \cite{mahoney2007method}. As
machine learning enters these new areas it is necessary for the
modeler to think beyond the simple objective of maximizing prediction
accuracy, and to consider the societal impact of their work.

% in these new ml fields, we cannot discriminate
% discrimination can happen in multiple ways
% - direct discrimination
For many of these applications, it is crucial to
ask if the predictions of a model are
\emph{fair}. For instance, imagine a bank wishes to predict if an individual should be given
a loan to buy a house. The bank wishes to use historical repayment data, alongside  individual data. If they simply learn a model
that  predicts whether the
loan will be paid back, it may unjustly favor
applicants of particular subgroups, due to past and
present prejudices. The Obama Administration released a report
describing this which urged data scientists to
analyze ``how technologies can deliberately or inadvertently
perpetuate, exacerbate, or mask
discrimination".\footnote{https://obamawhitehouse.archives.gov/blog/2016/05/04/big-risks-big-opportunities-intersection-big-data-and-civil-rights}

As a result, there has been immense interest in designing
algorithms that make fair predictions
\cite{hardt2016equality,dwork2012fairness,joseph2016rawlsian,kamishima2011fairness,zliobaite2015survey,zafar2016fairness,zafar2015learning,grgiccase,kleinberg2016inherent,calders2010three,kamiran2012data,bolukbasi2016man,kamiran2009classifying,zemel2013learning,louizos2015variational}. 
% Most
% of these works focus on formalizing fairness into a numeric
% definition and satisfying it with customized algorithms.
In large part, the initial work on fairness in machine learning has
focused on formalizing fairness into quantitative definitions and using them to solve a
discrimination problem in a certain dataset. Unfortunately, for a
practitioner, law-maker, judge, or anyone else who is interested in
implementing algorithms that control for discrimination, it can be
difficult to decide which definition of fairness to choose for the
task at hand. Indeed, we demonstrate that depending on the
relationship between a sensitive attribute and the data, certain
definitions of fairness can actually \emph{increase discrimination}.
In perhaps the most closely related work, CITET give similar arguments
but from a non-causal perspective.

% we propose a way to model data that allows a practitioner to assess what definitions of fairness are right for the problem at hand, and algorithms to ensure fairness
% OR
% we propose a way to interpret fairness...
% a) relationship between fairness and causality
% b) use pearl's models
% c) having an explicit model allows us to test fairness with the assumptions laid bare
% tension: Pearl already talks about discrimination, so we aren't really inventing new models. Are we even new in using these models to talk about fairness? Maybe... Pearl talks about variables that we might want to compute counterfactuals for in order to see if discrimination is happening.  
% Our proposal is:
% - situate a sensitive variable in a graph (not new).
% - Look at old definitions and see if anything bad could happen (new). 
% - Then define counterfactual fairness (new). 
% - Modeling helps us see where the weaknesses are in our assumptions and definitions (maybe not new)
% We don't want to see if every definition is counterfactually fair because then we're like everyone else, saying our definition is best
% 

% In this work,
We describe how techniques from causal inference can be effective tools for designing fair algorithms and argue, as in \citet{dedeo2014wrong}, that it is essential to properly address causality.
Specifically, we
% develop a technique to
leverage the causal framework of 
\citet{pearl2009causal} to model the relationship between sensitive
attributes and data. Our contributions are as follows:
\begin{enumerate}
    \item We model questions of fairness within a causal framework. This allows us to directly model \emph{how} unfairness affects the data at hand.
    \item We introduce \emph{counterfactual fairness}, which enforces that a distribution over possible predictions for an individual should remain unchanged, in a world where an individual's sensitive attribute had been different from birth.
    \item We analyze how enforcing existing definitions of fairness for different data may correspond or be in conflict with counterfactual fairness. In particular, we show that depending on the underlying state of the world some definitions of fairness may be inappropriate.
    \item We devise techniques for learning predictors that are counterfactually fair and demonstrate their use in several examples.
\end{enumerate}
%We demonstrate that by explicitly representing fairness within a causal model it becomes easy to critique different definitions of fairness as well the prediction methods that aim to accomplish these notions of fairness.




% RETHINK SPIN, ALWAYS RETHINK

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "ricardo_draft"
%%% End:
