%!TEX root=ricardo_draft.tex
% ml is now everywhere
Machine learning has spread to fields as diverse as credit scoring
(CITE), crime prediction (CITE), and loan assessment (CITE). As
machine learning enters these new areas it is necessary for the
modeler to think beyond the simple objective of maximizing prediction
accuracy, and to consider the social impact of their work.

% in these new ml fields, we cannot discriminate
% discrimination can happen in multiple ways
% - direct discrimination
In particular, for many of these new applications, it is crucial to
consider whether the predictions of a machine learning model are
\emph{fair}. For instance, imagine a bank wishes to train a machine
learning model to predict whether or not an individual should be given
a loan to buy a house. The bank wishes to use historical lending data
on whether or not a loan was paid back, along with personal
information on individuals. If the bank simply tries to learn a model
that accurately predicts who to loan to solely based on whether the
loan will be paid back, it may unjustly favor giving loans to
applicants of a particular demographic group, due to historical and
present prejudices. The Obama Administration released a report that
precisely describes this, and urged machine learning practitioners to
analyze ``how technologies can deliberately or inadvertently
perpetuate, exacerbate, or mask
discrimination"\footnote{https://obamawhitehouse.archives.gov/blog/2016/05/04/big-risks-big-opportunities-intersection-big-data-and-civil-rights}.

As a result, there has been immense interest recently in designing
machine learning algorithms that make fair
predictions~\cite{hardt2016equality,dwork2012fairness,joseph2016rawlsian,kamishima2011fairness,zliobaite2015survey,zafar2016fairness,zafar2015learning,grgiccase,kleinberg2016inherent,calders2010three,kamiran2012data,bolukbasi2016man,kamiran2009classifying,zemel2013learning,louizos2015variational}. Most
of these works focus on formalizing fairness into a numeric testable
definition, and which can be satisfied by bespoke algorithms.

In large part, the initial work on fairness in machine learning has
focused on formalizing the above definitions and using them to solve a
discrimination problem in a certain dataset. Unfortunately, for a
practitioner, law-maker, judge, or anyone else who is interested in
implementing algorithms that control for discrimination, it can be
difficult to decide which definition of fairness to choose for the
task at hand. Indeed, we demonstrate that depending on the
relationship between a sensitive attribute and the data certain
definitions of fairness can actually \emph{increase discrimination}.

% we propose a way to model data that allows a practitioner to assess what definitions of fairness are right for the problem at hand, and algorithms to ensure fairness
% OR
% we propose a way to interpret fairness...
% a) relationship between fairness and causality
% b) use pearl's models
% c) having an explicit model allows us to test fairness with the assumptions laid bare
% tension: Pearl already talks about discrimination, so we aren't really inventing new models. Are we even new in using these models to talk about fairness? Maybe... Pearl talks about variables that we might want to compute counterfactuals for in order to see if discrimination is happening.  
% Our proposal is:
% - situate a sensitive variable in a graph (not new).
% - Look at old definitions and see if anything bad could happen (new). 
% - Then define counterfactual fairness (new). 
% - Modeling helps us see where the weaknesses are in our assumptions and definitions (maybe not new)
% We don't want to see if every definition is counterfactually fair because then we're like everyone else, saying our definition is best
% 

In this work, we describe how techniques from causal inference can formalize questions of fair prediction. Specifically, we
develop a technique to leverage the causal models of Pearl
\cite{pearl2009causal} to model the relationship between the sensitive
attribute and data. Our contributions are as follows:
\begin{enumerate}
    \item We model questions of fairness within a causal framework. This allows us to directly model \emph{how} unfairness affects the data at hand.
    \item We introduce \emph{counterfactual fairness}, which enforces that a distribution over possible predictions for an individual should remain unchanged, in a world where an individual's sensitive attribute had been different from birth.
    \item We analyze how enforcing existing definitions of fairness for different data may or may not lead to fair predictions.
    \item We devise techniques learning predictors that are counterfactually fair.
\end{enumerate}
%We demonstrate that by explicitly representing fairness within a causal model it becomes easy to critique different definitions of fairness as well the prediction methods that aim to accomplish these notions of fairness.











% RETHINK SPIN, ALWAYS RETHINK

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "ricardo_draft"
%%% End:
